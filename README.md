# Multimedia_CS523_Project_3_SoundDCGAN
This repository is created as a deliverable for Project 3 for CS-523 course offered in Spring 2017 at University of Illinois at Chicago.

## 1. Project Members:
1. Kruti Sharma
2. Hengbin Li
3. Shreyas Kulkarni

# 2. Sound Generation Using Deep Convolutional Generative Adversarial Networks.
In this project we have tried to generate sound from DCGAN using 3 different approaches. Each approach is implemented in a separate interactive python file (ipynb) file. 

## 3. Running the Project
1. Download or clone the project on your local machine.
2. Copy all files from the 'Multimedia_CS523_Project_3_SoundDCGAN' folder into the root folder of jupyter notebook.
3. To Run the Piano_Music_Same.ipynb  make sure the 'piano' folder is copied into the root directory of jupyter notebook. This notebok describes approach 1 for generating music from Sound DCGAN.
4. To run the drum_beats.ipynb make sure the 'drum2beats' folder  is copied into the root directory of jupyter notebook and execute the notebook. This notebok describes approach 2 for generating music from Sound DCGAN.
5. To run the StringBeats.ipynb make sure the 'strings' folder is copied into the root directory of jupyter notebook and execute the notebook. This notebok describes approach 3 for generating music from Sound DCGAN.

### 3.1 Requirements to Run ipython notebook:
1. python 3.5 or Greater
2. Jupyter Notebook.

##### (Please note: Training the DCGAN is an optional step while running the python notebooks. The process of training the DCGAN is already done and is done through a separate python program and not is not done while running these python notebook. This notebook will expalin the steps to follow to train the DCGAN outside this python notebook). The code used to train the DCGAN is present in the folder 'SoundDCGAN' in which we have implemented a DCGAN model for generating greyscale images. The code written to convert images into tfrecords in present in 'create_tfrecords' folder. 

##### 3.2 Requirements if you want to train the DCGAN. Code Present in 'SoundDCGAN' folder (Training is optional step and user can follow the instructions in the python notebook's on how to train the DCGAN on the images generated from the sound files. Before training the images have to be converted into TfRecords (All instructions are given in the notebook))
1. python 3.5 or Greater
2. TensorFlow >= 1.0
3. appdirs==1.4.3
4. click==6.7
5. Flask==0.12
6. Flask-SQLAlchemy==2.2
7. itsdangerous==0.24
8. Jinja2==2.9.5
9. MarkupSafe==1.0
10. numpy==1.12.0
11. packaging==16.8
12. protobuf==3.2.0
13. pyparsing==2.2.0
14. six==1.10.0
15. SQLAlchemy==1.1.6
16. tensorflow==1.0.0
17. Werkzeug==0.12.1


## 4.  Approach 1: Piano_Music_Same.ipynb 
1. You can run this piano_music.ipynb notebook to see the sample outputs the DCGAN generated on piano music file. 
2. All the outputs and data required for running the file and training the DCGAN (training is an optional step) are present in the 'Piano' folder. 
3. All the instructions required for training if the user wants is also present in the Piano_Music_Same.ipynb. You can run the ipynb file to see the outputs and do not have to train.
4. Youtube video link: https://youtu.be/-Bf8nKTJu0o

### 4.1 Generating Music From Piano Music File.
1. This method corresponds to Approach 1 in our paper for generating sound from DCGAN.
2. The Music file used for this approach is piano music data file (sample_music.wav) present in the 'piano' folder.
3. In this process we are sampling the paino music file into samples of 1 second each and storing each seconds data as a greyscale png image. Since the file is stored at 44100 Hz per second. The size of the image will be 210 x 210.
4. In this process we are training the DCGAN with images of one second at one time. We are training the DCGAN 4 times.
5. The first time is training the DCGAN with image and image copies of the image generated from the first second of piano data, second time is training the DCGAN with image and image copies of the image generated from the 2nd second of piano data, third time is training the DCGAN with image and image copies of the image generated from the 3rd second of piano data, fourth time is training the DCGAN with image and image copies of the image generated from the 4th second of piano data.
6. For each separate training the DCGAN will produce images similar to the images of the second of sound clip it is trined on. We will conactenate the sound recovered from these 4 training images to respoduce a sound clip which is similar to the first 4 seconds of the sample_music.wav file.
7. This notebook describes the process used in converting the sample_music.wav file into greyscale images, training a DCGAN eachtime separately with the images of a single second and recovering the sound back from the images generated by the generator in two different ways.
##### (Please note: The process of training the DCGAN is done through a separate python program and not is not done in this python notebook. This notebook will expalin the steps to follow to train the DCGAN outside this python notebook). The code used to train the DCGAN is present in the folder 'SoundDCGAN'. We have implemented a DCGAN model for generating greyscale images. The code written to convert images into tfrecords in present in 'create_tfrecords' foleder. 

## 5. Approach 2: drum_beats.ipynb
1. You can run this drum_beats.ipynb notebook to see the sample outputs the DCGAN generated on drum beats music file. 
2. All the outputs and data required for running the file and training the DCGAN (training is an optional step) are present in the 'drum2beats' folder. 
3. All the instructions required for training if the user wants is also present in the drum_beats.ipynb. You can run the ipynb file to see the outputs and do not have to train. 
4. Youtube Video Link: https://www.youtube.com/watch?v=BEZbTEdwUnQ&feature=youtu.be

### 5.1 Generating Muisc From Drum Beats Music File.
1. This method corresponds to Approach 2 in our paper for generating sound from DCGAN.
2. The Music file used for this approach is drum beat music data file (drum2.wav). The main reason for using drum beats is because drums produce lower frequency sounds than string instruments and we wanted to understand if the frequency range makes any difference in results of a DCGAN while producing sound.
3. In this approach the drum2.wav file is spilt into 32 images where each image represents the frequencies in time corresposning to one beat.
4. The wav file is 21 seconds long and thus has 32 beats in the 21 seconds because the ffdrum2.wav file is recorded at 92 beats per minute.
5. The notebook describes the process used in convering the drum2.wav file into greyscale images, training a DCGAN with the images and recovering the sound back from the images generated by the generator.

##### (Please note: The process of training the DCGAN is done through a separate python program and not is not done in this python notebook. This notebook will expalin the steps to follow to train the DCGAN outside this python notebook). The code used to train the DCGAN is present in the folder 'SoundDCGAN'. We have implemented a DCGAN model for generating greyscale images. The code written to convert images into tfrecords in present in 'create_tfrecords' foleder.

## 6. Approach 3: StringBeats.ipynb
1. You can run this StringBeats.ipynb notebook to see the sample outputs the DCGAN generated on drum beats music file. 
2. All the outputs and data required for running the file and training the DCGAN (training is an optional step) are present in the 'strings' folder. 
3. All the instructions required for training if the user wants is also present in the StringBeats.ipynb. You can run the ipynb file to see the outputs and do not have to train.
4. Youtube Video Link: https://youtu.be/8j0wmop08Tg
### 6.1 Generating Music From String Instrument Music File.

1. This method corresponds to Approach 3 in our paper for generating sound from DCGAN.
2. The Music file used for this approach is string instrument music data file (string.wav). The main reason for using string instruments is because string instruments have high frequency sounds and we wanted to understand if the DCGAN's are better at producing sounds of high frequencies.
3. In this approach the string.wav file is converted into 16 images each of size 164 x 164, where each image represents the frequencies in time a little less than 1 beat.
4. The wav file is 12 seconds and is recorded at 80 beats per minute. We clip the frequqncy data of the file from 0th position to 430336 position. This crops the length of string music file to just under 10 seconds. This enables us to convert the sound frequency data in 16 images each of size 164 x 164 as 164 x 164 x16 = 430336.
5. The notebook describes the process used in convering the string.wav file into greyscale images, training a DCGAN with the images and recovering the sound back from the images generated by the generator.

##### (Please note: The process of training the DCGAN is done through a separate python program and not is not done in this python notebook. This notebook will expalin the steps to follow to train the DCGAN outside this python notebook). The code used to train the DCGAN is present in the folder 'SoundDCGAN'. We have implemented a DCGAN model for generating greyscale images. The code written to convert images into tfrecords in present in 'create_tfrecords' foleder.

