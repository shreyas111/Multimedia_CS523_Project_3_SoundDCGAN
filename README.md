# Multimedia_CS523_Project_3_SoundDCGAN
This repository is created as a deliverable for Project 3 for CS-523 course offered in Spring 2017 at University of Illinois at Chicago.

## Project Members:

1. Kruti Sharma
2. Hengbin Li
3. Shreyas Kulkarni

# Sound Generation Using Deep Convolutional Generative Adversarial Networks.
In this project we have tried to generate sound from DCGAN using 3 different approaches. Each approach is implemented in a separate interactive python file (ipynb) file. 

## Running the Project

1. Run Each .ipynb file to see the outputs we obtained for each approach. All instructions are present in the .ipynb files.
2. Apporach 1 for generating music from Sound DCGAN is explained in Piano_Music_Same.ipynb
3. Approach 2 for generating music from Sound DCGAN is explained in drum_beats.ipynb
4. Approach 3 for generating music from Sound DCGAN is explained in StringBeats.ipynb

## Approach 1: Piano_Music_Same.ipynb 
1. You can run this piano_music.ipynb notebook to see the sample outputs the DCGAN generated on piano music file. 
2. All the outputs and data required for running the file and training the DCGAN (training is an optional step) are present in the 'Piano' folder. 
3. All the instructions required for training if the user wants is also present in the Piano_Music_Same.ipynb. You can run the ipynb file to see the outputs and do not have to train.

### Generating Muisc From Piano Music File.
1. This method corresponds to Approach 1 in our paper for generating sound from DCGAN.
2. The Music file used for this approach is piano music data file (sample_music.wav) present in the 'piano' folder.
3. In this process we are sampling the paino music file into samples of 1 second each and storing each seconds data as a greyscale png image. Since the file is stored at 44100 Hz per second. The size of the image will be 210 x 210.
4. In this process we are training the DCGAN with images of one second at one time. We are training the DCGAN 4 times.
5. The first time is training the DCGAN with image and image copies of the image generated from the first second of piano data, second time is training the DCGAn with image and image copies of the image generated from the 2nd second of piano data, third time is training the DCGAn with image and image copies of the image generated from the 3rd second of piano data, fourth time is training the DCGAn with image and image copies of the image generated from the 4th second of piano data.
6. For each separate training the DCGAN will produce images similar to the images of the second of sound clip it is trined on. We will conactenate the sound recovered from these 4 training images to respoduce a sound clip which is similar to the first 4 seconds of the sample_music.wav file.
7. Following describes the process used in converting the sample_music.wav file into greyscale images, training a DCGAN eachtime separately with the images of a single second and recovering the sound back from the images generated by the generator in two different ways.
#### (Please note: The process of training the DCGAN is done through a separate python program and not is not done in this python notebook. This notebook will expalin the steps to follow to train the DCGAN outside this python notebook).

## Approach 2: drum_beats.ipynb
You can run this drum_beats.ipynb notebook to see the sample outputs the DCGAN generated on drum beats music file. All the outputs for generated using this approach are present in the 'drum2beats' folder. 

## Approach 3: StringBeats.ipynb
You can run this StringBeats.ipynb notebook to see the sample outputs the DCGAN generated on drum beats music file. All the outputs for generated using this approach are present in the 'strings' folder.
