#Import Statements
#This file is used to read the input data from .tfrecords files present in the data_dir.
#The data is read from the .tfrecords files and passed to the DCGAN for training by creating a session of tf and running the model
#defined in the DCGAN.py file. 
#Every 500 iterations the file saves the checkpoints for the generator and discriminator model in the logdir directory and every 100 iterations the image generated 
#by the generator is saved in the images_dir

import os
import time
from datetime import datetime

import tensorflow as tf

from dcgan import DCGAN

FLAGS = tf.app.flags.FLAGS

tf.app.flags.DEFINE_string('logdir', 'logdir/',
                           """logs and checkpoint data are saved in this directory.""")
tf.app.flags.DEFINE_string('images_dir', 'images',
                           """Images generated by the generator are saved in this directory""")
tf.app.flags.DEFINE_string('data_dir', 'data',
                           """Path to read the data from. The data has to be in .tfrecord format.""")
tf.app.flags.DEFINE_integer('num_examples_per_epoch_for_train', 5000,
                            """Number of examples per epoch""")
tf.app.flags.DEFINE_integer('max_steps', 90001,
                            """Maximum number of steps to run the training.""")

#Size of input images: INPUT_IMAGE_SIZE X INPUT_IMAGE_SIZE
INPUT_IMAGE_SIZE = 164
#Defining cropped image size
#Cropping the input images to the size: crop_image_size X crop_image_size
CROP_IMAGE_SIZE = 164

#Function used to read the input greyscale images from the .tfrecord files present in the data_dir.
def inputs(batch_size, s_size):
    print('batch_size:', batch_size)
    print('s_size', s_size)
    print('path is:', os.path.join(FLAGS.data_dir))
    #Picking all the files ending in .tfrecord
    files = [os.path.join(FLAGS.data_dir, f) for f in os.listdir(FLAGS.data_dir) if f.endswith('.tfrecord')]
    print(files)
    #Using string_input_producer() to output the files into a queue which can then be used for an input pipeline. 
    fqueue = tf.train.string_input_producer(files)
    print(fqueue)
    #Using the TFRecordReader to output records from a .tfrecord file.
    reader = tf.TFRecordReader()
    #Using the read method of the TFRecordReader() class to read the next record from the queue. 
    _, value = reader.read(fqueue)
    #From the value extracting a particular feature with key 'image/encoded'.
    features = tf.parse_single_example(value, features={'image/encoded': tf.FixedLenFeature([], tf.string)})
    #change for greyscale changed from 3 to 1 chanels value. 
    #Decoding the feature as an .png image and casting it as an image. Since we want to retrieve a greyscale image the number of channels=1
    image = tf.cast(tf.image.decode_png(features['image/encoded'], channels=1), tf.float32)
    # Resizing the image to the size of the cropped image
    image = tf.image.resize_image_with_crop_or_pad(image, CROP_IMAGE_SIZE, CROP_IMAGE_SIZE)
    image = tf.image.random_flip_left_right(image)

    min_queue_examples = FLAGS.num_examples_per_epoch_for_train
    # shuffle the images into batchs of batch sizes.
    images = tf.train.shuffle_batch(
        [image],
        batch_size=batch_size,
        capacity=min_queue_examples + 3 * batch_size,
        min_after_dequeue=min_queue_examples)
    print('Images are:', images)
    tf.summary.image('images', images)
    return tf.subtract(tf.div(tf.image.resize_images(images, [s_size * 2 ** 4, s_size * 2 ** 4]), 127.5), 1.0)


def main(argv=None):
    #Creating an object of the DCGAN
    dcgan = DCGAN(s_size=10)
    #Call to function to read data in .tfrecord format
    traindata = inputs(dcgan.batch_size, dcgan.s_size)
    print('Train data', traindata)
    # Calculating the losses
    losses = dcgan.loss(traindata)
    # Extracting the Generator and Discrimintor loss
    tf.summary.scalar('g loss', losses[dcgan.g])
    tf.summary.scalar('d loss', losses[dcgan.d])
    #Minimize the Generator and the Discrimintor losses
    train_op = dcgan.train(losses)
    summary_op = tf.summary.merge_all()

    #Creating objects to save the generator and discriminator states
    g_saver = tf.train.Saver(dcgan.g.variables)
    d_saver = tf.train.Saver(dcgan.d.variables)
    #Defining the directory to store the generator check points
    g_checkpoint_path = os.path.join(FLAGS.logdir, 'gckpt/')
    print('G checkpoint path: ', g_checkpoint_path)
    ##Defining the directory to store the discriminator check points
    d_checkpoint_path = os.path.join(FLAGS.logdir, 'dckpt/')
    print('D Checkpoint Path:',d_checkpoint_path)

    if not os.path.exists(g_checkpoint_path):
        os.makedirs(g_checkpoint_path)

    if not os.path.exists(d_checkpoint_path):
        os.makedirs(d_checkpoint_path)

    with tf.Session() as sess:
        summary_writer = tf.summary.FileWriter(FLAGS.logdir, graph=sess.graph)
        newStepNo = 0
        # restore or initialize generator
        sess.run(tf.global_variables_initializer())

        gckpt = tf.train.get_checkpoint_state(g_checkpoint_path)
        if gckpt and gckpt.model_checkpoint_path:
            g_saver.restore(sess, gckpt.model_checkpoint_path)
            print('Model restored from ' + gckpt.model_checkpoint_path)
            newStepCheck = gckpt.model_checkpoint_path
            newStepNo = int(newStepCheck.split('-')[1])
        #if os.path.exists(g_checkpoint_path):
            #print('restore variables for G:')
            #for v in dcgan.g.variables:
                #print('  ' + v.name)
            #g_saver.restore(sess, ckpt.model_checkpoint_path)
            #g_saver.restore(sess, g_checkpoint_path)
        #if os.path.exists(d_checkpoint_path):
            #print('restore variables for D:')
            #for v in dcgan.d.variables:
              #  print('  ' + v.name)
            #d_saver.restore(sess, d_checkpoint_path)

        dckpt = tf.train.get_checkpoint_state(d_checkpoint_path)
        if dckpt and dckpt.model_checkpoint_path:
            d_saver.restore(sess, dckpt.model_checkpoint_path)
            print('Model restored from ' + dckpt.model_checkpoint_path)

        # setup for monitoring
        sample_z = sess.run(tf.random_uniform([dcgan.batch_size, dcgan.z_dim], minval=-1.0, maxval=1.0))
        images = dcgan.sample_images(1, 1, inputs=sample_z)

        # start training
        tf.train.start_queue_runners(sess=sess)

        #for step in range(FLAGS.max_steps):
        while newStepNo <= FLAGS.max_steps:
            start_time = time.time()
            _, g_loss, d_loss = sess.run([train_op, losses[dcgan.g], losses[dcgan.d]])
            duration = time.time() - start_time
            print('{}: step {:5d}, loss = (G: {:.8f}, D: {:.8f}) ({:.3f} sec/batch)'.format(
                datetime.now(), newStepNo, g_loss, d_loss, duration))

            # save generated images
            if newStepNo % 100 == 0:
                # summary
                summary_str = sess.run(summary_op)
                summary_writer.add_summary(summary_str, newStepNo)
                # sample images
                filename = os.path.join(FLAGS.images_dir, '%05d.png' % newStepNo)
                with open(filename, 'wb') as f:
                    f.write(sess.run(images))
            # save variables
            if newStepNo % 500 == 0:
                g_saver.save(sess, g_checkpoint_path + 'g.ckpt', global_step=newStepNo)
                print('Save mode for G in checkpoint_path: ', g_checkpoint_path)
                d_saver.save(sess, d_checkpoint_path + 'd.ckpt', global_step=newStepNo)
                print('Save mode for D in checkpoint_path: ', d_checkpoint_path)
                #g_saver.save(sess, g_checkpoint_path, global_step=step)
                #d_saver.save(sess, d_checkpoint_path, global_step=step)
            newStepNo = newStepNo+1


if __name__ == '__main__':
    tf.app.run()
